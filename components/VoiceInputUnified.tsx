"use client";

/**
 * VoiceInputUnified - çµ±åˆéŸ³å£°å…¥åŠ›ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
 *
 * è‡ªå‹•ã§ãƒ™ã‚¹ãƒˆãªæ–¹æ³•ã‚’é¸æŠ:
 * - Chrome/Edge: Web Speech APIï¼ˆç„¡æ–™ï¼‰
 * - Safari iOS: Whisper APIï¼ˆæœ‰æ–™ã ãŒiOSã§å‹•ãï¼‰
 *
 * ä½¿ç”¨ä¾‹:
 * ```tsx
 * <VoiceInputUnified
 *   onTranscript={(text) => console.log(text)}
 *   onInterimTranscript={(text) => console.log('é€”ä¸­:', text)}
 * />
 * ```
 */

import { useState, useRef, useCallback, useEffect } from "react";

interface VoiceInputUnifiedProps {
  onTranscript: (text: string) => void;
  onInterimTranscript?: (text: string) => void;
  onStartListening?: () => void;
  onStopListening?: () => void;
  onError?: (error: string) => void;
  whisperEndpoint?: string;
  disabled?: boolean;
  maxDuration?: number;
  className?: string;
  // ã‚³ã‚¹ãƒˆåˆ¶å¾¡
  forceWhisper?: boolean; // å¼·åˆ¶çš„ã«Whisperä½¿ç”¨
  allowWhisper?: boolean; // Whisperãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨±å¯ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆtrueï¼‰
}

type InputState = "idle" | "listening" | "recording" | "processing";
type InputMethod = "webspeech" | "whisper" | "none";

// Web Speech API å‹å®šç¾©
interface SpeechRecognitionEvent extends Event {
  results: SpeechRecognitionResultList;
  resultIndex: number;
}

interface SpeechRecognitionErrorEvent extends Event {
  error: string;
}

export function VoiceInputUnified({
  onTranscript,
  onInterimTranscript,
  onStartListening,
  onStopListening,
  onError,
  whisperEndpoint = "/api/transcribe",
  disabled = false,
  maxDuration = 60,
  className = "",
  forceWhisper = false,
  allowWhisper = true,
}: VoiceInputUnifiedProps) {
  const [state, setState] = useState<InputState>("idle");
  const [method, setMethod] = useState<InputMethod>("none");
  const [duration, setDuration] = useState(0);

  // Refs
  const recognitionRef = useRef<any>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const timerRef = useRef<NodeJS.Timeout | null>(null);
  const streamRef = useRef<MediaStream | null>(null);

  // åˆ©ç”¨å¯èƒ½ãªæ–¹æ³•ã‚’åˆ¤å®š
  useEffect(() => {
    if (typeof window === "undefined") {
      setMethod("none");
      return;
    }

    // å¼·åˆ¶Whisperãƒ¢ãƒ¼ãƒ‰
    if (forceWhisper && allowWhisper) {
      setMethod("whisper");
      return;
    }

    // Web Speech API ãƒã‚§ãƒƒã‚¯
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (SpeechRecognition) {
      // iOS Safariåˆ¤å®šï¼ˆWeb Speech APIã‚ã‚‹ãŒåˆ¶é™ã‚ã‚Šï¼‰
      const isIOS = /iPad|iPhone|iPod/.test(navigator.userAgent);
      const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);

      if (isIOS && isSafari && allowWhisper) {
        // iOS Safariã¯Whisperæ¨å¥¨
        setMethod("whisper");
      } else {
        // ãã‚Œä»¥å¤–ã¯Web Speech API
        setMethod("webspeech");
      }
    } else if (allowWhisper) {
      // Web Speech APIéå¯¾å¿œ â†’ Whisper
      setMethod("whisper");
    } else {
      setMethod("none");
    }
  }, [forceWhisper, allowWhisper]);

  // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
  useEffect(() => {
    return () => {
      if (timerRef.current) clearInterval(timerRef.current);
      if (streamRef.current) {
        streamRef.current.getTracks().forEach((track) => track.stop());
      }
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
    };
  }, []);

  // ========================================
  // Web Speech API
  // ========================================
  const startWebSpeech = useCallback(() => {
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      onError?.("Web Speech APIãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“");
      return;
    }

    const recognition = new SpeechRecognition();
    recognition.lang = "ja-JP";
    recognition.continuous = true;
    recognition.interimResults = true;

    recognition.onstart = () => {
      setState("listening");
      onStartListening?.();
    };

    recognition.onresult = (event: SpeechRecognitionEvent) => {
      let interimTranscript = "";
      let finalTranscript = "";

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (finalTranscript) {
        onTranscript(finalTranscript);
      }
      if (interimTranscript) {
        onInterimTranscript?.(interimTranscript);
      }
    };

    recognition.onerror = (event: SpeechRecognitionErrorEvent) => {
      console.error("Speech recognition error:", event.error);
      if (event.error !== "aborted") {
        onError?.(`éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼: ${event.error}`);
      }
      setState("idle");
    };

    recognition.onend = () => {
      setState("idle");
      onStopListening?.();
    };

    recognitionRef.current = recognition;
    recognition.start();
  }, [onTranscript, onInterimTranscript, onStartListening, onStopListening, onError]);

  const stopWebSpeech = useCallback(() => {
    if (recognitionRef.current) {
      recognitionRef.current.stop();
    }
  }, []);

  // ========================================
  // Whisper API
  // ========================================
  const startWhisper = useCallback(async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: { echoCancellation: true, noiseSuppression: true },
      });
      streamRef.current = stream;

      const mimeType = MediaRecorder.isTypeSupported("audio/webm")
        ? "audio/webm"
        : "audio/mp4";

      const mediaRecorder = new MediaRecorder(stream, { mimeType });
      mediaRecorderRef.current = mediaRecorder;
      chunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) chunksRef.current.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        stream.getTracks().forEach((track) => track.stop());
        setState("processing");

        const blob = new Blob(chunksRef.current, { type: mimeType });
        const reader = new FileReader();

        reader.onloadend = async () => {
          const base64 = (reader.result as string).split(",")[1];

          try {
            const response = await fetch(whisperEndpoint, {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({ audio: base64, mimeType }),
            });

            const result = await response.json();

            if (result.success && result.data?.text) {
              onTranscript(result.data.text);
            } else {
              onError?.(result.error?.message || "æ–‡å­—èµ·ã“ã—å¤±æ•—");
            }
          } catch (err) {
            onError?.(err instanceof Error ? err.message : "ã‚¨ãƒ©ãƒ¼");
          } finally {
            setState("idle");
            setDuration(0);
            onStopListening?.();
          }
        };
        reader.readAsDataURL(blob);
      };

      mediaRecorder.start(1000);
      setState("recording");
      setDuration(0);
      onStartListening?.();

      timerRef.current = setInterval(() => {
        setDuration((prev) => {
          if (prev + 1 >= maxDuration) stopWhisper();
          return prev + 1;
        });
      }, 1000);
    } catch (err) {
      onError?.("ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸ");
      setState("idle");
    }
  }, [whisperEndpoint, maxDuration, onTranscript, onError, onStartListening, onStopListening]);

  const stopWhisper = useCallback(() => {
    if (timerRef.current) {
      clearInterval(timerRef.current);
      timerRef.current = null;
    }
    if (mediaRecorderRef.current && state === "recording") {
      mediaRecorderRef.current.stop();
    }
  }, [state]);

  // ========================================
  // çµ±åˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼
  // ========================================
  const handleClick = useCallback(() => {
    if (state === "idle") {
      if (method === "webspeech") {
        startWebSpeech();
      } else if (method === "whisper") {
        startWhisper();
      }
    } else if (state === "listening") {
      stopWebSpeech();
    } else if (state === "recording") {
      stopWhisper();
    }
  }, [state, method, startWebSpeech, stopWebSpeech, startWhisper, stopWhisper]);

  // æ™‚é–“ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
  const formatDuration = (sec: number) =>
    `${Math.floor(sec / 60)}:${(sec % 60).toString().padStart(2, "0")}`;

  // éå¯¾å¿œ
  if (method === "none") {
    return (
      <button disabled className={`opacity-50 ${className}`} title="éå¯¾å¿œ">
        ğŸ¤
      </button>
    );
  }

  return (
    <div className="relative inline-flex items-center">
      <button
        onClick={handleClick}
        disabled={disabled || state === "processing"}
        className={`
          relative inline-flex items-center justify-center
          w-10 h-10 rounded-full transition-all duration-200
          ${state === "idle" ? "bg-gray-100 hover:bg-gray-200" : ""}
          ${state === "listening" ? "bg-green-500 animate-pulse" : ""}
          ${state === "recording" ? "bg-red-500 animate-pulse" : ""}
          ${state === "processing" ? "bg-blue-500" : ""}
          ${disabled ? "opacity-50 cursor-not-allowed" : "cursor-pointer"}
          ${className}
        `}
        title={
          state === "idle"
            ? `ã‚¯ãƒªãƒƒã‚¯ã§é–‹å§‹ (${method === "webspeech" ? "ç„¡æ–™" : "Whisper"})`
            : state === "listening" || state === "recording"
            ? "ã‚¯ãƒªãƒƒã‚¯ã§åœæ­¢"
            : "å‡¦ç†ä¸­..."
        }
      >
        {state === "idle" && <span className="text-lg">ğŸ¤</span>}
        {state === "listening" && <span className="text-white text-lg">ğŸ¤</span>}
        {state === "recording" && <span className="text-white text-lg">â¹</span>}
        {state === "processing" && <span className="text-white animate-spin">â³</span>}
      </button>

      {/* éŒ²éŸ³æ™‚é–“è¡¨ç¤º (Whisperãƒ¢ãƒ¼ãƒ‰) */}
      {state === "recording" && (
        <span className="ml-2 text-xs text-red-500 font-mono">
          {formatDuration(duration)}
        </span>
      )}

      {/* æ–¹å¼è¡¨ç¤º (é–‹ç™ºç”¨ã€æœ¬ç•ªã§ã¯æ¶ˆã™) */}
      {process.env.NODE_ENV === "development" && state === "idle" && (
        <span className="ml-1 text-[10px] text-gray-400">
          {method === "webspeech" ? "free" : "$"}
        </span>
      )}
    </div>
  );
}

export default VoiceInputUnified;
